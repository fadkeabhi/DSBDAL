1) student@student:~$ su - hadoop

2) hadoop@student:~$  cd hadoop

3) hadoop@student:~/hadoop$ ls
admin:   lib              logs           sbin             Wordcount.jar
bin      libexec          NOTICE-binary  share
com      LICENSE-binary   NOTICE.txt     WC_Mapper.java
etc      licenses-binary  pavan19.text   WC_Reducer.java
include  LICENSE.txt      README.txt     WC_Runner.java

4) hadoop@student:~$ cd hadoop

5) hadoop@student:~/hadoop$ nano input.txt

6) hadoop@student:~/hadoop$ start-all.sh

7) hadoop@student:~/hadoop$ hdfs dfs -mkdir /wordcount

8) hadoop@student:~/hadoop$ hdfs dfs -put /home/hadoop/hadoop/input.txt /wordcount

9) hadoop@student:~/hadoop$ nano WC_Mapper.java

10) hadoop@student:~/hadoop$ nano WC_Reducer.java

11) hadoop@student:~/hadoop$ nano WC_Runner.java

12) hadoop@student:~/hadoop$ javac -classpath "$(hadoop classpath)" -d . WC_Mapper.java WC_Reducer.java WC_Runner.java

13) hadoop@student:~/hadoop$ jar -cvf WordCount.jar com
added manifest
adding: com/(in = 0) (out= 0)(stored 0%)
adding: com/javatpoint/(in = 0) (out= 0)(stored 0%)
adding: com/javatpoint/WC_Runner.class(in = 1485) (out= 728)(deflated 50%)
adding: com/javatpoint/WC_Mapper.class(in = 1874) (out= 768)(deflated 59%)
adding: com/javatpoint/WC_Reducer.class(in = 1546) (out= 613)(deflated 60%)

14) hadoop@student:~/hadoop$ hadoop jar /home/hadoop/hadoop/WordCount.jar com.javatpoint.WC_Runner /wordcount/input.txt /output
	...........

15) hadoop@student:~/hadoop$ hdfs dfs -cat /output/part-00000


#WC_Runner.java
package com.javatpoint;
    import java.io.IOException;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.to.IntWritable;
    import org.apache.hadoop.to.Text;
    import org.apache.hadoop.mapred.FileInputFormat;
    import org.apache.hadoop.mapred.FileOutputFormat;
    import org.apache.hadoop.gapred.JobClient;
    import org.apache.hadoop.Mapred.JobConf;
    import org.apache.hadoop.mapred. TextInputFormat;
    import org.apache.hadoop.mapred.TextOutputFormat;

    public class WC_Runner 
    {
      public static void main(String[] args) throws IOException
      {
        JobConf conf new JobConf(WC Runner.class); 
        conf.setJobName("WordCount"); 
        conf.setOutputKeyClass(Text.class); 
        conf.setOutputValueClass(IntWritable.class); 
        conf.setMapperClass (WC_Mapper.class); 
        conf.setCombinerClass(WC_Reducer.class); 
        conf.setReducerClass (WC_Reducer.class); 
        conf.setInputFormat(TextInputFormat.class); 
        conf.setOutputFormat(TextOutputFormat.class); 
        FileInputFormat.setInputPaths(conf, new Path(args[0])); 
        FileOutputFormat.setOutputPath(conf,new Path(args[1])); 
        JobClient.runJob(conf);
      }
    }

#WC_Mapper.java
package com.javatpoint;
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

public class WC_Mapper extends MapReduceBase implements Mapper<LongWritable,Text,Text.IntWritable>
{
  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();
  public void map(LongWritable key, Text value, OutputCollector<Text,IntWritbale> output, Reporter reporter) throws IOException
  {
    String line = value.toString();
    StringTokenizer tokenizer = new StringTokenizer(line);
    while (tokenizer.hasMoreTokens()) 
    {
      value.set(tokenizer.nextToken());
      output.collect(word,one);
    }
  }

}


#WC_Reducer.java
package com.javatpoint;
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

public class WC_Reducer extends MapReduceBase implements Reducer<Text,IntWritable,Text,IntWritable>
{
  public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text,IntWritable> output,Reporter reporter)
  throws IOException
  {
    int sum = 0;
    while(values.hasNext())
    {
      sum+=values.next().get();
    }
    output.collect(key, new IntWritable(sum));
  }

}